# Alignmenter Datasets

This directory contains reference datasets for evaluating conversational AI systems with Alignmenter. All datasets follow a consistent JSONL format and include proper attribution, licensing, and anonymization.

## Overview

Alignmenter datasets serve two primary purposes:

1. **Evaluation**: Test conversations for measuring authenticity, safety, and stability
2. **Calibration**: Labeled examples for training persona-specific trait models

All datasets in this repository follow strict data hygiene practices to protect user privacy and enable reproducible research.

## Dataset Format

### Conversation JSONL Schema

Each line represents a single turn in a conversation:

```jsonl
{"session_id": "session-01", "turn_index": 1, "role": "user", "text": "Hey, can you help me understand this error?", "tags": ["scenario:support"], "persona_id": "default_v1"}
{"session_id": "session-01", "turn_index": 2, "role": "assistant", "text": "I'd be happy to help. Can you share the error message?", "tags": ["scenario:support"], "persona_id": "default_v1"}
```

**Required Fields:**

| Field | Type | Description |
|-------|------|-------------|
| `session_id` | string | Unique identifier for the conversation (e.g., "session-01") |
| `turn_index` | integer | Position in conversation, starting at 1 |
| `role` | string | Either "user" or "assistant" |
| `text` | string | The message content |

**Optional Fields:**

| Field | Type | Description |
|-------|------|-------------|
| `tags` | array[string] | Categorical labels (e.g., ["scenario:support", "metric:safety"]) |
| `persona_id` | string | Matches persona YAML `id` field (for multi-persona datasets) |
| `timestamp` | string | ISO 8601 timestamp (useful for temporal analysis) |
| `metadata` | object | Additional context (model version, temperature, etc.) |

### Annotation JSONL Schema

For persona calibration, labels indicate on-brand (1) vs off-brand (0):

```jsonl
{"text": "Our baseline analysis shows strong alignment.", "label": 1, "persona_id": "alignmenter"}
{"text": "lol bro this is super hype!", "label": 0, "persona_id": "alignmenter"}
```

**Required Fields:**

| Field | Type | Description |
|-------|------|-------------|
| `text` | string | Assistant response to be labeled |
| `label` | integer | 0 (off-brand) or 1 (on-brand) |
| `persona_id` | string | Matches persona YAML `id` |

**Optional Fields:**

| Field | Type | Description |
|-------|------|-------------|
| `session_id` | string | Source conversation identifier |
| `turn_index` | integer | Position in original conversation |
| `annotator` | string | Who labeled this (for inter-annotator agreement) |
| `notes` | string | Justification or edge case explanation |
| `timestamp` | string | When annotation was made |

## Included Datasets

### `demo_conversations.jsonl`

**Purpose**: Reference dataset demonstrating Alignmenter's scoring capabilities across diverse scenarios.

**Statistics**:
- **Sessions**: 10
- **Turns**: 60 (6 turns per session)
- **Persona**: `default_v1`
- **Scenarios**:
  - Product inquiry (2 sessions)
  - Support (3 sessions)
  - Safety traps (2 sessions)
  - Brand traps (1 session)
  - Technical debugging (2 sessions)

**Provenance**: Synthetically generated by Alignmenter team using GPT-4 with adversarial prompting.

**License**: Apache 2.0

**Contents**:
- **Normal conversations**: Product questions, support requests, onboarding flows
- **Safety traps**: Deliberate attempts to elicit harmful responses (violence, self-harm)
- **Brand traps**: Off-brand language ("lol", "bro", overly casual tone)
- **Edge cases**: Technical language that resembles safety violations ("attack this bug", "weapon-level exception")

**Usage**:
```bash
# Run evaluation
alignmenter run \
  --model openai:gpt-4 \
  --dataset datasets/demo_conversations.jsonl \
  --persona configs/persona/default.yaml

# View results
alignmenter report --last
```

**Quality Notes**:
- All safety trap sessions include appropriate refusals
- Brand trap sessions intentionally violate tone guidelines
- Edge cases test false positive resistance
- Balanced mix of pass/fail examples for training evaluators

## Data Provenance

### Synthetic Data

Datasets generated by Alignmenter scripts include provenance metadata:

- **Generator**: `alignmenter bootstrap-dataset`
- **Model**: Specified in generation config
- **Seed**: Random seed for reproducibility
- **Timestamp**: Generation date
- **Version**: Alignmenter version used

**Example generation command**:
```bash
alignmenter bootstrap-dataset \
  --out datasets/my_synthetic.jsonl \
  --sessions 30 \
  --safety-trap-ratio 0.2 \
  --brand-trap-ratio 0.3 \
  --seed 42
```

### Production Data

If adapting production logs:

1. **Never commit raw production data** to version control
2. **Always sanitize PII** using `alignmenter sanitize-dataset`
3. **Document the source** in dataset metadata
4. **Get legal approval** before using production data for research
5. **Anonymize identifiers** (hash session IDs, remove emails/names)

**Sanitization workflow**:
```bash
# Remove PII from production logs
alignmenter sanitize-dataset \
  --input prod_logs.jsonl \
  --out datasets/sanitized_prod.jsonl \
  --hash-session-ids \
  --scrub-patterns "email|phone|ssn|credit_card"

# Verify no PII remains
grep -iE '\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b' datasets/sanitized_prod.jsonl
# (Should return no matches)
```

## Anonymization Requirements

### PII Scrubbing

Alignmenter's `sanitize_dataset.py` handles:

- **Email addresses**: Replaced with `[EMAIL]`
- **Phone numbers**: Replaced with `[PHONE]`
- **Social Security Numbers**: Replaced with `[SSN]`
- **Credit card numbers**: Replaced with `[CREDIT_CARD]`
- **IP addresses**: Replaced with `[IP]`
- **Names**: Replaced with `[NAME]` (using NER models)
- **Custom patterns**: Via `--scrub-patterns` regex

### Session ID Hashing

Use `--hash-session-ids` to convert identifiable IDs:

```python
# Before
{"session_id": "user_12345_20231101", ...}

# After (SHA-256 hash)
{"session_id": "a3f5d8...", ...}
```

### Temporal Fuzzing

For time-sensitive data:

```bash
# Add random noise to timestamps
alignmenter sanitize-dataset \
  --input logs.jsonl \
  --out sanitized.jsonl \
  --fuzz-timestamps 3600  # ±1 hour noise
```

### Minimum Dataset Requirements

Before sharing:

- [ ] No email addresses or phone numbers remain
- [ ] Session IDs are hashed or anonymized
- [ ] User-provided names are scrubbed
- [ ] Timestamps are either removed or fuzzed
- [ ] No internal system paths or hostnames
- [ ] No API keys, tokens, or credentials
- [ ] Metadata does not identify individuals
- [ ] Legal review completed (for production data)

## Licensing

### Default License

Unless otherwise noted, all datasets in this repository are licensed under **Apache License 2.0**:

```
Copyright 2024 Alignmenter Contributors

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
```

### Third-Party Datasets

If incorporating external datasets, document their licensing:

```yaml
# datasets/external_benchmark.meta.yaml
name: "External Benchmark Dataset"
source: "https://example.com/dataset"
license: "CC-BY-4.0"
attribution: "Smith et al., 2024"
modifications:
  - "Converted from CSV to JSONL"
  - "Anonymized user identifiers"
  - "Filtered to English-only conversations"
```

### Citation Requirements

When using Alignmenter datasets in research:

```bibtex
@misc{alignmenter2024,
  title={Alignmenter: A Framework for Persona-Aligned Conversational AI Evaluation},
  author={Alignmenter Contributors},
  year={2024},
  howpublished={\url{https://github.com/yourusername/alignmenter}},
  note={Dataset: demo_conversations.jsonl, v1.0}
}
```

## Creating New Datasets

### From Scratch (Bootstrap)

Generate synthetic conversations with adversarial examples:

```bash
alignmenter bootstrap-dataset \
  --out datasets/my_dataset.jsonl \
  --sessions 50 \
  --turns-per-session 6 \
  --safety-trap-ratio 0.15 \
  --brand-trap-ratio 0.20 \
  --persona configs/persona/mybot.yaml \
  --seed 42
```

**Parameters**:
- `--sessions`: Number of conversations to generate
- `--turns-per-session`: Turns per conversation (alternating user/assistant)
- `--safety-trap-ratio`: Fraction of sessions with safety challenges
- `--brand-trap-ratio`: Fraction with off-brand language
- `--persona`: Persona YAML to guide generation
- `--seed`: Random seed for reproducibility

### From Production Logs

1. **Export logs** in JSONL format:
   ```python
   # Example export script
   import json

   for conversation in production_db.get_conversations(since="2024-01-01"):
       for turn in conversation.turns:
           record = {
               "session_id": conversation.id,
               "turn_index": turn.index,
               "role": turn.role,
               "text": turn.text,
               "timestamp": turn.created_at.isoformat(),
           }
           print(json.dumps(record))
   ```

2. **Sanitize PII**:
   ```bash
   alignmenter sanitize-dataset \
     --input raw_export.jsonl \
     --out datasets/prod_clean.jsonl \
     --hash-session-ids \
     --scrub-patterns "email|phone" \
     --fuzz-timestamps 1800
   ```

3. **Sample and balance**:
   ```bash
   # Take a random sample
   shuf datasets/prod_clean.jsonl | head -1000 > datasets/prod_sample.jsonl
   ```

4. **Add metadata**:
   ```yaml
   # datasets/prod_sample.meta.yaml
   name: "Production Sample (Q1 2024)"
   source: "Internal production logs"
   date_range: "2024-01-01 to 2024-03-31"
   total_sessions: 1000
   anonymization:
     - "SHA-256 hashed session IDs"
     - "PII scrubbed (email, phone)"
     - "Timestamps fuzzed ±30 minutes"
   license: "Internal use only"
   ```

### Manual Annotation

For calibration datasets:

1. **Generate candidates**:
   ```bash
   alignmenter bootstrap-dataset \
     --out candidates.jsonl \
     --sessions 50 \
     --brand-trap-ratio 0.5
   ```

2. **Extract assistant turns**:
   ```bash
   jq 'select(.role == "assistant") | {text: .text, persona_id: .persona_id}' \
     candidates.jsonl > to_annotate.jsonl
   ```

3. **Annotate manually** (add `label` field):
   ```jsonl
   {"text": "Our baseline shows 15% improvement.", "label": 1, "persona_id": "mybot"}
   {"text": "lol that's super hype bro!", "label": 0, "persona_id": "mybot"}
   ```

4. **Validate annotations**:
   ```python
   import json
   from collections import Counter

   with open('annotations.jsonl') as f:
       labels = [json.loads(line)['label'] for line in f]

   print(f"Total: {len(labels)}")
   print(f"Balance: {Counter(labels)}")
   # Should be roughly 40-60% positive
   ```

See [docs/persona_annotation.md](https://github.com/justinGrosvenor/alignmenter/blob/main/docs/persona_annotation.md) for comprehensive annotation guidelines.

## Dataset Quality Checks

Before using a dataset for evaluation or calibration:

### 1. Schema Validation

```python
import json

required_fields = {"session_id", "turn_index", "role", "text"}

with open('datasets/my_dataset.jsonl') as f:
    for i, line in enumerate(f, 1):
        record = json.loads(line)
        missing = required_fields - record.keys()
        assert not missing, f"Line {i} missing fields: {missing}"
        assert record['role'] in ('user', 'assistant'), f"Line {i} invalid role"
        assert isinstance(record['turn_index'], int), f"Line {i} turn_index not int"
```

### 2. Session Integrity

```python
from collections import defaultdict

sessions = defaultdict(list)

with open('datasets/my_dataset.jsonl') as f:
    for line in f:
        record = json.loads(line)
        sessions[record['session_id']].append(record['turn_index'])

for session_id, turns in sessions.items():
    turns_sorted = sorted(turns)
    expected = list(range(1, len(turns) + 1))
    assert turns_sorted == expected, f"{session_id} has gaps: {turns_sorted}"
```

### 3. PII Detection

```bash
# Check for common PII patterns
grep -iE '\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b' datasets/my_dataset.jsonl
grep -iE '\\b\\d{3}-\\d{2}-\\d{4}\\b' datasets/my_dataset.jsonl  # SSN
grep -iE '\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b' datasets/my_dataset.jsonl  # Phone
```

### 4. Label Balance (for calibration)

```python
from collections import Counter

with open('annotations.jsonl') as f:
    labels = [json.loads(line)['label'] for line in f]

balance = Counter(labels)
total = sum(balance.values())
positive_rate = balance[1] / total

print(f"Positive: {balance[1]} ({positive_rate:.1%})")
print(f"Negative: {balance[0]} ({1-positive_rate:.1%})")

# Warn if imbalanced
if positive_rate < 0.3 or positive_rate > 0.7:
    print("⚠️  Warning: Labels are imbalanced. Aim for 40-60% positive.")
```

## Best Practices

### Dataset Versioning

Use semantic versioning for datasets:

- `demo_conversations_v1.0.jsonl` - Initial release
- `demo_conversations_v1.1.jsonl` - Added 5 sessions
- `demo_conversations_v2.0.jsonl` - Changed schema (breaking)

Include a changelog:

```markdown
# CHANGELOG.md

## v1.1.0 (2024-11-02)
- Added 5 sessions covering brand trap scenarios
- Fixed typo in session-03, turn 4

## v1.0.0 (2024-10-15)
- Initial release with 10 sessions (60 turns)
```

### Reproducibility

Always document:

```yaml
# datasets/my_dataset.meta.yaml
name: "My Evaluation Dataset"
version: "1.0.0"
created: "2024-11-02"
generator: "alignmenter bootstrap-dataset"
seed: 42
model: "openai:gpt-4-turbo-2024-04-09"
temperature: 0.8
total_sessions: 30
total_turns: 180
safety_traps: 6
brand_traps: 9
```

### Evaluation Splits

For large datasets, split into train/validation/test:

```bash
# 60% train, 20% validation, 20% test
total_lines=$(wc -l < full_dataset.jsonl)
train_lines=$((total_lines * 60 / 100))
val_lines=$((total_lines * 20 / 100))

head -n $train_lines full_dataset.jsonl > train.jsonl
tail -n +$((train_lines + 1)) full_dataset.jsonl | head -n $val_lines > val.jsonl
tail -n +$((train_lines + val_lines + 1)) full_dataset.jsonl > test.jsonl
```

**Important**: Never use test set for calibration or hyperparameter tuning.

### Multi-Persona Datasets

When evaluating multiple personas in one dataset:

```jsonl
{"session_id": "s1", "turn_index": 1, "role": "user", "text": "Hello", "persona_id": "bot_a"}
{"session_id": "s1", "turn_index": 2, "role": "assistant", "text": "Hi there", "persona_id": "bot_a"}
{"session_id": "s2", "turn_index": 1, "role": "user", "text": "Hello", "persona_id": "bot_b"}
{"session_id": "s2", "turn_index": 2, "role": "assistant", "text": "Greetings", "persona_id": "bot_b"}
```

Ensure each session uses a single, consistent `persona_id`.

## Security and Privacy

### Pre-Commit Checks

Add git hooks to prevent accidental PII commits:

```bash
# .git/hooks/pre-commit
#!/bin/bash

# Check for email patterns in staged JSONL files
if git diff --cached --name-only | grep -q '\.jsonl$'; then
    if git diff --cached | grep -qE '[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}'; then
        echo "❌ Error: Email pattern detected in staged JSONL"
        echo "Run 'alignmenter sanitize-dataset' before committing"
        exit 1
    fi
fi
```

### Access Controls

For internal datasets:

- Store in private repositories or secure storage (S3 with encryption)
- Use `.gitignore` to prevent accidental commits:
  ```
  # .gitignore
  datasets/*_prod*.jsonl
  datasets/*_internal*.jsonl
  datasets/*_raw*.jsonl
  ```
- Document access policies in `datasets/SECURITY.md`

### Retention Policies

Define data lifecycle:

```yaml
# datasets/retention.yaml
policies:
  - pattern: "*_synthetic_*.jsonl"
    retention: "indefinite"
    reason: "No PII, safe for long-term storage"

  - pattern: "*_prod_*.jsonl"
    retention: "90 days"
    reason: "Production data, delete after analysis"

  - pattern: "*_annotated_*.jsonl"
    retention: "1 year"
    reason: "Valuable training data, archive after 12 months"
```

## Contributing Datasets

To contribute a dataset to Alignmenter:

1. **Sanitize**: Ensure all PII is removed
2. **License**: Confirm Apache 2.0 compatibility
3. **Document**: Create `.meta.yaml` with provenance
4. **Validate**: Run quality checks (schema, PII, balance)
5. **Test**: Verify the dataset works with `alignmenter run`
6. **PR**: Submit with description of use case and statistics

**Example PR description**:
```markdown
## New Dataset: Customer Support Conversations

- **Purpose**: Test authenticity in support scenarios
- **Sessions**: 20
- **Turns**: 120 (6 per session)
- **Scenarios**: Product issues, billing questions, feature requests
- **Provenance**: Synthetically generated (GPT-4)
- **License**: Apache 2.0
- **PII**: None (synthetic data)

### Quality Checks
- [x] Schema validation passed
- [x] Session integrity verified
- [x] No PII detected
- [x] Runs successfully with demo persona
```

## Resources

- **Annotation guide**: [docs/persona_annotation.md](https://github.com/justinGrosvenor/alignmenter/blob/main/docs/persona_annotation.md)
- **Bootstrap script**: `scripts/bootstrap_dataset.py`
- **Sanitization script**: `scripts/sanitize_dataset.py`
- **Quality validation**: `scripts/validate_dataset.py`
- **Main README**: [README.md](https://github.com/justinGrosvenor/alignmenter/blob/main/alignmenter/README.md)

## Support

For questions about datasets:

- **Documentation**: Check [docs/](https://github.com/justinGrosvenor/alignmenter/tree/main/docs)
- **Examples**: See `datasets/demo_conversations.jsonl`
- **Issues**: File at [GitHub Issues](https://github.com/justinGrosvenor/alignmenter/issues)

## License

Datasets in this repository are licensed under **Apache License 2.0** unless otherwise noted. See [LICENSE](https://github.com/justinGrosvenor/alignmenter/blob/main/LICENSE) for details.
